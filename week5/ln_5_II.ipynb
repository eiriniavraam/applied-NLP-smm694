{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5, part II\n",
    "\n",
    "# ―Sentiment, affect, and connotation\n",
    "\n",
    "<img src=\"images/_1.jpeg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Naive Bayes Classifier (NBC) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ So called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact.\n",
    "+ The intuition of the classifier is shown in the right-displayed figure:\n",
    "  * text documents are represented as bag-of-words:\n",
    "    - the position of words doesn't matter\n",
    "    - what matters is the frequency of words in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/_2.png\" width=\"100%\">\n",
    "\n",
    "Source is Jurafsky & Martin, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Intuition of the Naive Bayes Classifier\n",
    "\n",
    "For a document $d$, out of all classes $c \\in C$, the classifier returns the \n",
    "class $\\hat c$ which has the maximum posterior probability given the document:\n",
    "\n",
    "\\begin{equation}\\label{eq:}\n",
    "\\hat c = argmaxP*(c|d)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " This idea of Bayesian inference has been known since the work of Bayes* (1763),\n",
    "and was first applied to text classification by Mosteller and Wallace (1964). \n",
    "\n",
    "The intuition of Bayesian classification is to use Bayes’ rule to transform Eq. \n",
    "1 into other probabilities that have some useful properties. Bayes’ rule is \n",
    "presented in three other probabilities:\n",
    "\n",
    "\\begin{equation}\n",
    "P(x|y) = \\frac{P(y|x)P(x)}{P(y)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\* Bayes is interred in Bunhill Fields Cemetery\n",
    "\n",
    "<img src=\"images/_3.jpeg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bayesian inference (cont'd)\n",
    "\n",
    "Eq. [2] can be substituted into Eq. [1] to get Eq. [3]:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat c = argmax \\frac{P(d|c)P(c)}{P(d)}\n",
    "\\end{equation}\n",
    "\n",
    "with $c \\in C$\n",
    "\n",
    "As $P(d)$ doesn't change for each class, Eq. [3] can be dispensed as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat c = argmax P(d|c)P(c)\n",
    "\\end{equation}\n",
    "\n",
    "with $c \\in C$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bayesian inference (cont'd)\n",
    "\n",
    "Eq. [4] contains two probabilities:\n",
    "\n",
    "+ $P(c)$ is the prior probability of the class $c$\n",
    "+ $P(d|c)$ is the likelihood of the document, which can also be expressed as:\n",
    "  \n",
    "  \\begin{equation}\n",
    "  \\hat c = argmaxP(f_{1}, f_{2}, ..., f_{n}|c)P(c)\n",
    "  \\end{equation}\n",
    "  \n",
    "In practice, Eq. [5] is just too costly/impossible to estimate:\n",
    "+ estimating the probability of every possible combination of features would require:\n",
    "  * huge numbers of parameters\n",
    "  * impossibly large training sets\n",
    "+ NBCs therefore make two simplifying assumptions:\n",
    "  * 'bag of words' assumption ― the order of words doesn't matter\n",
    "    - the vector of features $F$ encodes word identities not positions\n",
    "  * 'naive Bayes assumption': the probabilities $P(f_{i}|c)$ are independent\n",
    "    given the class $c$\n",
    "    \n",
    "    $P(f_{1}, f_{2}, ..., f_{n}) = P(f_{1}|c) \\cdot P(f_{2}|c) \\cdot ... \n",
    "      \\cdot P(f_{n}|c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bayesian inference (cont'd)\n",
    "\n",
    "The final equation for the class chosen by a NBC is thus:\n",
    "\n",
    "\\begin{equation}\n",
    "c_{NB} = argmaxP(c) \\Pi P(f|c)\n",
    "\\end{equation}\n",
    "\n",
    "with $c \\in C$ and $f \\in F$\n",
    "\n",
    "As Jurafsky and Martin (2019) note, Naive Bayes calculations, like calculations for \n",
    "language modeling, are done in log space, to avoid underflow and increase speed. Hence,\n",
    "Eq. [6] can dispensed as follows [7]:\n",
    "\n",
    "\\begin{equation}\n",
    "c_{NB} = argmax log P(c) + \\sum log P(w_{i}|c)\n",
    "\\end{equation}\n",
    "\n",
    "with $c \\in C$ and $i \\in V$, where $V$ is the dictionary of the corpus of text"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
