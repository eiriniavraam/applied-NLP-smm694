{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 2, part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Representing words and meanings\n",
    "- Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/_99.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Meanings are central to natural language \n",
    "================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Key **emerging properties** of natural language:\n",
    "\n",
    "+ language is socially-oriented\n",
    "+ words reflect symbols and social categories (that is, culture)\n",
    "+ language convey (ambiguous) meanings\n",
    "\n",
    "The **concept of meaning** is the place to start for any natural language processing analyses.\n",
    "\n",
    "Let's have a closer look at:\n",
    "\n",
    "+ how 'meanings' are represented in computational linguistics\n",
    "+ how machines look at 'meanings'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/_0.jpg\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How to represent the meaning of a word?\n",
    "=====================================\n",
    "\n",
    "A (computational) linguist's perspective\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Two pillars that reflect how linguists' think about meanings:\n",
    "\n",
    "+ denotational semantics\n",
    "+ distributional hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The intuition behind denotational semantics\n",
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Semantics, as the study of meanings, concerns the relationship between signifiers ― like words, phrases, signs, and symbols ― and what they stand for in reality, their denotation.\n",
    "\n",
    "Denotations comprise both the salient features associated with an entity (being a concrete instance or a category) and the cognitive and behavioral effects of using a signifier that invokes an entity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Example: the lexeme 'hip-hop' conveys meanings about what constitute a 'hip-hop' song as well as the values, norms, and beliefs that orient the behavior of 'hip-hop people.'\n",
    "\n",
    "<img src=\"images/_2.jpg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The distributional hypothesis (DH)\n",
    "============================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*''Difference of meaning correlates with difference of distribution''*\n",
    "\n",
    "―Harris, 1954\n",
    "\n",
    "*''Semantic similarity is a function of the contexts in which words are used.''*\n",
    "\n",
    "―Miller & Charles, 1951\n",
    "\n",
    "*''DS is not only a method for lexical analysis but also a theoretical framework to build computational models of semantic memory''*\n",
    "\n",
    "―Lenci, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "DH lies at the hearth of vector space models.\n",
    "\n",
    "<img src=\"images/_1.png\" width=\"100%\">\n",
    "\n",
    "Fig. 1 ― Distributional vectors of the lexemes car, cat, dog, and van. Notes: source is 'Lenci 2018 ― ARL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Distributional representations\n",
    "========================\n",
    "\n",
    "The distributional representation of a lexical item is typically a distributional vector representing its co-occurrences with linguistic contexts ― hence the name vector space semantics.\n",
    "\n",
    "The kind of co-occurrence relation between target and context lexemes determines different\n",
    "types of collocates and distributional representations.\n",
    "\n",
    "Context types (Firth (1957): (You shall know a word) by the company it keeps!\n",
    "\n",
    "| Context types                                | Co-occurrences             |\n",
    "| -------------------------------------------- | -------------------------- |\n",
    "| Undirected window-based collocate            | $word$                     |\n",
    "| Directed window-based collocate              | $\\langle R, word \\rangle$  |\n",
    "| Dependency-filtered syntactic collocate [(see spaCy's documentation)](https://spacy.io/displacy-3504502e1d5463ede765f0a789717424.svg) | word                       |\n",
    "| Dependency-typed syntactic collocate         | $\\langle obj, word \\rangle$|\n",
    "| Text region                                  | Firth (1957)               |\n",
    "\n",
    "Notes: source is 'Lenci 2018 ― ARL'\n",
    "\n",
    "    [1]: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Building distributional representations (1/3)\n",
    "====================================\n",
    "\n",
    "The basic method of building distributional vectors consists of the following procedure:\n",
    "\n",
    "+ co-occurrences between lexical items and linguistic contexts are extracted from a corpus and counted\n",
    "+ the distribution of lexical items is represented with a co-occurrence matrix, whose rows correspond to target lexical items, columns to contexts, and the entries to their co-occurrence frequency\n",
    "+ raw frequencies are then usually transformed into significance weights to reflect the importance of the contexts\n",
    "+ the semantic similarity between lexemes is measured with the similarity between their row vectors in the co-occurrence matrix\n",
    "\n",
    "Suppose we have extracted and counted the co-occurrences of the targets $T =\\{bike, car, dog, lion\\}$ with the context lexemes $C =\\{bite, buy, drive, eat, get, live, park, ride, tell\\}$ in a corpus. Their distribution is represented with the following co-occurrence matrix $MT x C$,in which mt,c is the co-occurrence frequency of t with $c$:\n",
    "\n",
    "<img src=\"images/_4.png\" width=\"70%\">\n",
    "\n",
    "Notes: source is 'Lenci 2018 ― ARL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Building distributional representations (2/3)\n",
    "====================================\n",
    "\n",
    "The most common weighting function in DS is positive pointwise mutual information (PPMI) (Bullinaria & Levy 2007).\n",
    "\n",
    "PPMI measures how much the probability of a target–context pair estimated in the training corpus is higher than the probability we should expect if the target and the context occurred independently of one another.\n",
    "\n",
    "Matrix 3 contains the PPMI weights computed from the raw co- occurrence frequencies in matrix 1\n",
    "\n",
    "\\begin{equation}\n",
    "PPMI(t,c) = max \\bigg( 0, log_{2} \\frac{p(t,c)}{p(t)p(c)} \\bigg )\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"images/_5.png\" width=\"70%\">\n",
    "\n",
    "Notes: source is 'Lenci 2018 ― ARL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Building distributional representations (3/3)\n",
    "====================================\n",
    "\n",
    "The distributional similarity between two lexemes u and v is measured with the similarity\n",
    "between their distributional vectors u and v.\n",
    "\n",
    "Once we have computed the pairwise distributional similarity between the targets, we can identify the k nearest neighbors of each target t, that is, the k lexical items with the highest similarity score with t. The cosine is the most popular measure of vector similarity in DS:\n",
    "\n",
    "\\begin{equation}\n",
    "cos(u,v) = \\frac{u \\cdot v}{\\Vert u \\Vert \\Vert v \\Vert} \n",
    "\\end{equation}\n",
    "\n",
    "The cosine ranges from 1 for identical vectors to −1 (0, if the vectors do not contain negative values).matrix reports the cosines between the row vectors in matrix 3:\n",
    "\n",
    "<img src=\"images/_6.png\" width=\"35%\">\n",
    "\n",
    "Notes: source is 'Lenci 2018 ― ARL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Distributional semantics and NLP frameworks/tools\n",
    "==========================================\n",
    "\n",
    "<img src=\"images/_7.png\" width=\"90%\">\n",
    "\n",
    "Notes: source is 'Lenci 2018 ― ARL'"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
