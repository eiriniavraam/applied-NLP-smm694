{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 2, part IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Representing words and meanings\n",
    "- Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/_99.jpg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Is there a statistical or machine learning approach that might work in place of an annotated ontology or a pattern-matching approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Provided we have access to a reasonably large (and diverse) corpus of text, how can we represent the duality between words and meanings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Statistical framework route\n",
    "======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ Traditional NLP\n",
    "    - Bag-of-Words\n",
    "    - One-hot encodings\n",
    "\n",
    "+ Modern NLP\n",
    "    - Embeddings (e.g., a vector space generated via $\\texttt{word2vec}$) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/_8.jpg\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bag-of-Words (BoW)\n",
    "==========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given some text corpora $D$, a BoW work-flow implies the following steps:\n",
    "\n",
    "1. $\\forall d \\in D$ (e.g, documents, state- ments, sentences, and even single words), get the the token $\\Phi(d)$\n",
    "2. $\\forall s \\in S$ (i.e., unique lexical items) and $\\forall d \\in D$, get the cardinality $\\vert s \\vert$\n",
    "\n",
    "We call the possible vectors a machine might create this way a vector space.\n",
    "\n",
    "Such a vector space allows us to use linear algebra (and libraries such as NumPy, Scipy, or Numba) to manipulate lexical items and compute things like distances and statistics involving natural language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/_12.png\" width=\"100%\">\n",
    "\n",
    "Token sorting tray (source is Lane, Howard & Hapke 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What can we do with BoW data?\n",
    "==========================\n",
    "\n",
    "For example, we can address search queries such as: *''What is the combination of words most likely to follow a particular bag of words?''* Or, if a user enters a sequence of words, *''What is the closest bag of words in our database to a bag-of-words vector provided by the user?''*\n",
    "\n",
    "General take-home on BoW:\n",
    "\n",
    "+ a BoW approach can generate meaningful responses to answers \n",
    "+ in a BoW approach, humans do not pass any rules to machines (you remember pattern-matching?)\n",
    "+ BoW leverages distributional data to appreciate the semantic similarity of lexical items\n",
    "\n",
    "Warning: a BoW approach doesn't say anything about the specific meanings of lexical items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BoW in action\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'is': 3,\n",
      "         'not': 2,\n",
      "         'Success': 1,\n",
      "         'final;': 1,\n",
      "         'failure': 1,\n",
      "         'fatal:': 1,\n",
      "         'It': 1,\n",
      "         'the': 1,\n",
      "         'courage': 1,\n",
      "         'to': 1,\n",
      "         'continue': 1,\n",
      "         'that': 1,\n",
      "         'counts.': 1,\n",
      "         '-Winston': 1,\n",
      "         'S.': 1,\n",
      "         'Churchill': 1})\n"
     ]
    }
   ],
   "source": [
    "# let's import Counter, a special kind of\n",
    "# dictionary that computes the cardinality of\n",
    "# elements\n",
    "from collections import Counter\n",
    "\n",
    "# sample sentence\n",
    "sentence = \"\"\"\n",
    "Success is not final;\n",
    "failure is not fatal:\n",
    "It is the courage to continue that counts. \n",
    "-Winston S. Churchill\n",
    "\"\"\"\n",
    "\n",
    "# tokenization\n",
    "tokens = sentence.split()\n",
    "\n",
    "# BoW\n",
    "bow = Counter(tokens)\n",
    "\n",
    "# print\n",
    "from pprint import pprint\n",
    "pprint(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One-hot vectors \n",
    "=============\n",
    "\n",
    "One of the main limitations of the BoW approach is the proliferation of unique vectors to compare and contrast.\n",
    "\n",
    "One-hot vectors (a form of discrete representation of lexical items) mitigate the curse of dimensionality by considering whether a word is or is not present in a piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# let's import numpy to manipulat the text\n",
    "import numpy as np\n",
    "\n",
    "# sample sentence\n",
    "sentence = \"\"\"\n",
    "Success is not final; failure is not fatal:\n",
    "It is the courage to continue that counts. \n",
    "-Winston S. Churchill\n",
    "\"\"\"\n",
    "\n",
    "# tokenization\n",
    "tokens = str.split(sentence)\n",
    "\n",
    "# vocabulary (unique words)\n",
    "vocab = sorted(set(tokens))\n",
    "', '.join(vocab)\n",
    "\n",
    "# count of tokens\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "# size of vocabulary\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# one-hot vector representation\n",
    "# -- empty np array\n",
    "onehot_vectors = np.zeros((num_tokens, vocab_size), int)\n",
    "# -- fill-in values\n",
    "for i, word in enumerate(tokens):\n",
    "    onehot_vectors[i, vocab.index(word)] = 1\n",
    "    \n",
    "# print np array\n",
    "pprint(onehot_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -Winston Churchill It S. Success continue counts. courage failure fatal:  \\\n",
      "0                                 1                                           \n",
      "1                                                                             \n",
      "2                                                                             \n",
      "3                                                                             \n",
      "4                                                                  1          \n",
      "5                                                                             \n",
      "6                                                                             \n",
      "7                                                                         1   \n",
      "8                      1                                                      \n",
      "9                                                                             \n",
      "10                                                                            \n",
      "11                                                         1                  \n",
      "12                                                                            \n",
      "13                                         1                                  \n",
      "14                                                                            \n",
      "15                                                 1                          \n",
      "16        1                                                                   \n",
      "17                        1                                                   \n",
      "18                  1                                                         \n",
      "\n",
      "   final; is not that the to  \n",
      "0                             \n",
      "1          1                  \n",
      "2              1              \n",
      "3       1                     \n",
      "4                             \n",
      "5          1                  \n",
      "6              1              \n",
      "7                             \n",
      "8                             \n",
      "9          1                  \n",
      "10                      1     \n",
      "11                            \n",
      "12                         1  \n",
      "13                            \n",
      "14                  1         \n",
      "15                            \n",
      "16                            \n",
      "17                            \n",
      "18                            \n"
     ]
    }
   ],
   "source": [
    "# some embellishments\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(onehot_vectors, columns=vocab)\n",
    "df[df ==0] = ''\n",
    "pprint(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Limitations of one-hot encodings\n",
    "===========================\n",
    "\n",
    "There is no natural notion of similarity for one-hot vectors! (Mannings, 2019)\n",
    "\n",
    "**Example 1:** the vectors associated with 'good' and 'fine' are orthogonal:\n",
    "\n",
    "```\n",
    "good = [0, 0, 1, 0, 0, 0]\n",
    "\n",
    "fine = [0, 0, 0, 0, 1, 0]\n",
    "```\n",
    "\n",
    "**Example 2:** 'greasy spoon' and 'British cafe' express the same category of eatery but the intersection of their one-hot vectors is empty.\n",
    "\n",
    "```\n",
    "greasy spoon = [[0, 1, 0,  0], [0, 0, 1, 0]]\n",
    "\n",
    "British cafe = [[1, 0, 0, 0], [0, 0, 0, 1]]\n",
    "```\n",
    "\n",
    "Shall we try to use WordNet’s list of synonyms to get similarity? Likely as not, a bad idea...WordNet has severe limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Modern NLP: Distributional Hypothesis + DL\n",
    "===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From DH to word vectors\n",
    "=====================\n",
    "\n",
    "According to the Distributional Hypothesis, a focal word’s $\\omega$ meaning is a function of the linguistic context ― i.e., the lexical items in the neighborhood of the focal word.\n",
    "\n",
    "Then, considering (all) the many contexts of $\\omega$ (e.g., regulation) helps to create an accurate vector representation of $\\omega$.\n",
    "\n",
    "Sample sentences containing the word 'regulation':\n",
    "\n",
    "```\n",
    "... to encourage and implement the adoption of common REGULATIONs for all forms of motor sports and series across the ...\n",
    "\n",
    "countries should adhere to the cost-benefit paradigm of REGULATION, forcing bureaucrats to outline all the benefits of ...\n",
    "\n",
    "Agencies create REGULATIONs (also known as \"rules\") under the authority of Congress to help ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Word vectors as dense, real valued vectors\n",
    "===================================\n",
    "\n",
    "Ultimately, by observing and analyzing a same word in multiple context, we aim at building a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts.\n",
    "\n",
    "Below is a portion of the [vector](https://spacy.io/usage/vectors-similarity) associated with the word 'banana'.\n",
    "\n",
    "```\n",
    "array([2.02280000e-01,  -7.66180009e-02,   3.70319992e-01,\n",
    "       3.28450017e-02,  -4.19569999e-01,   7.20689967e-02,\n",
    "      -3.74760002e-01,   5.74599989e-02,  -1.24009997e-02,\n",
    "       5.29489994e-01,  -5.23800015e-01,  -1.97710007e-01,\n",
    "      -3.41470003e-01,   5.33169985e-01,  -2.53309999e-02,\n",
    "       1.73800007e-01,   1.67720005e-01,   8.39839995e-01,\n",
    "       5.51070012e-02,   1.05470002e-01,   3.78719985e-01,\n",
    "       2.42750004e-01,   1.47449998e-02,   5.59509993e-01,\n",
    "       1.25210002e-01,  -6.75960004e-01,   3.58420014e-01,\n",
    "       # ... and so on ...\n",
    "       3.66849989e-01,   2.52470002e-03,  -6.40089989e-01,\n",
    "      -2.97650009e-01,   7.89430022e-01,   3.31680000e-01,\n",
    "      -1.19659996e+00,  -4.71559986e-02,   5.31750023e-01], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Overview of the $\\texttt{word2vec}$ algorithm\n",
    "================================\n",
    "\n",
    "$\\texttt{word2vec}$ (Mikolov et al. 2013) is a framework for learning word vectors\n",
    "\n",
    "Idea ― given a corpus of text $D$:\n",
    "\n",
    "+ each word $d$ is associated with a vector \n",
    "+ go through each position $k$ in the text, which has a center word $\\omega$ and context words $\\eta$\n",
    "+ use the similarity of the word vectors for $\\omega$ and $\\eta$ to calculate the probability of $\\eta$ given $\\omega$ (or vice versa)\n",
    "+ keep adjusting the word vectors to maximize this probability\n",
    "\n",
    "Source is Manning 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Next week, we'll focus on $\\texttt{word2vec}$ and word vectors only. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
