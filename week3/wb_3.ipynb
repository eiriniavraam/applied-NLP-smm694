{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#How-to-train-embeddings?\" data-toc-modified-id=\"How-to-train-embeddings?-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>How to train embeddings?</a></span></li><li><span><a href=\"#Training-$\\texttt{word2vec}$-embedings-with-Gensim\" data-toc-modified-id=\"Training-$\\texttt{word2vec}$-embedings-with-Gensim-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Training $\\texttt{word2vec}$ embedings with Gensim</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-data\" data-toc-modified-id=\"Training-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Training data</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#NLP-pipeline\" data-toc-modified-id=\"NLP-pipeline-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>NLP pipeline</a></span></li><li><span><a href=\"#Get-bigrammed-sentences\" data-toc-modified-id=\"Get-bigrammed-sentences-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Get bigrammed-sentences</a></span></li><li><span><a href=\"#Implement-the-$\\texttt{word2vec}$-with-Gensim\" data-toc-modified-id=\"Implement-the-$\\texttt{word2vec}$-with-Gensim-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Implement the $\\texttt{word2vec}$ with Gensim</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup-the-params\" data-toc-modified-id=\"Setup-the-params-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Setup the params</a></span></li><li><span><a href=\"#Build-the-vocabulary\" data-toc-modified-id=\"Build-the-vocabulary-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Build the vocabulary</a></span></li><li><span><a href=\"#Train-the-model\" data-toc-modified-id=\"Train-the-model-2.5.3\"><span class=\"toc-item-num\">2.5.3&nbsp;&nbsp;</span>Train the model</a></span></li></ul></li></ul></li><li><span><a href=\"#Explore-the-model\" data-toc-modified-id=\"Explore-the-model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Explore the model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Do-vectors-make-any-sense?\" data-toc-modified-id=\"Do-vectors-make-any-sense?-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Do vectors make any sense?</a></span></li><li><span><a href=\"#Closer-characters?\" data-toc-modified-id=\"Closer-characters?-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Closer characters?</a></span></li></ul></li><li><span><a href=\"#Statistical-post-processing-of-word-vectors\" data-toc-modified-id=\"Statistical-post-processing-of-word-vectors-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Statistical post-processing of word vectors</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train embeddings?\n",
    "\n",
    "Here are some Pythonic options (many others exist):\n",
    "\n",
    "+ Tensorflow\n",
    "+ Pytorch\n",
    "+ Gensim\n",
    "\n",
    "<span class=\"mark\">Note:</span>:\n",
    "\n",
    "+ training embeddings falls beyond the remit spaCy!\n",
    "+ $\\texttt{word2vec}$, Fasttext, and GloVe are algorithms, they're not pieces of software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training $\\texttt{word2vec}$ embedings with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play a little bit with [funny data...](https://www.kaggle.com/shilpibhattacharyya/the-big-bang-theory-dataset/data?select=big_bang_theory_dataset.csv)\n",
    "\n",
    "<img src=\"images/_13.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reada data\n",
    "\n",
    "# --+ load libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# --+ set path\n",
    "in_f = os.path.join('data', 'big_bang_theory_dataset.csv')\n",
    "\n",
    "# --+ create a df\n",
    "df = pd.read_csv(in_f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>The apartment</td>\n",
       "      <td>The apartment</td>\n",
       "      <td>The room in the basement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scene</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sheldon enters, takes out a box, takes a bean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text</th>\n",
       "      <td>Again I’m right here.</td>\n",
       "      <td>Fine. The record shall so reflect. Now getting...</td>\n",
       "      <td>One two three four five six seven eight… Drat....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Speaker</th>\n",
       "      <td>Leonard</td>\n",
       "      <td>Sheldon</td>\n",
       "      <td>Sheldon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Season</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0  \\\n",
       "Unnamed: 0                      0   \n",
       "Location            The apartment   \n",
       "Scene                         NaN   \n",
       "Text        Again I’m right here.   \n",
       "Speaker                   Leonard   \n",
       "Season                          3   \n",
       "\n",
       "                                                            1  \\\n",
       "Unnamed: 0                                                  1   \n",
       "Location                                        The apartment   \n",
       "Scene                                                     NaN   \n",
       "Text        Fine. The record shall so reflect. Now getting...   \n",
       "Speaker                                               Sheldon   \n",
       "Season                                                      5   \n",
       "\n",
       "                                                            2  \n",
       "Unnamed: 0                                                  2  \n",
       "Location                             The room in the basement  \n",
       "Scene        Sheldon enters, takes out a box, takes a bean...  \n",
       "Text        One two three four five six seven eight… Drat....  \n",
       "Speaker                                               Sheldon  \n",
       "Season                                                      6  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview\n",
    "df.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37688 entries, 0 to 38917\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       37688 non-null  int64 \n",
      " 1   text     37688 non-null  object\n",
      " 2   speaker  37688 non-null  object\n",
      " 3   season   37688 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# some cleaning\n",
    "\n",
    "# --+ rename cols\n",
    "old_cols = df.columns\n",
    "new_cols = ['id', 'location', 'scene', 'text', 'speaker', 'season']\n",
    "df.rename(dict(zip(old_cols, new_cols)), axis=1, inplace=True)\n",
    "\n",
    "# --+ drop redundant columns\n",
    "df.drop(['location', 'scene'], axis=1, inplace=True)\n",
    "\n",
    "# --+ remove NaNs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# --+ ... and check (fine, it's a small-ish dataset, but let's a give try)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\",\n",
    "                    datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "# nlp pipeline\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs as list\n",
    "\n",
    "# --+ some cleaning\n",
    "def cleaning(_string):\n",
    "    '''\n",
    "    : argument: string\n",
    "    : return  : string\n",
    "    '''\n",
    "    # purge non alpha characters\n",
    "    alpha = re.sub(\"[^A-Za-z']+\", ' ', str(_string))\n",
    "    return alpha.lower()\n",
    "\n",
    "                   \n",
    "# --+ get clean text\n",
    "docs = [cleaning(item) for item in df.text.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pipeline\n",
    "nlp = spacy.load('en_core_web_lg', disable=['ner', 'parser', 'tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized text\n",
    "docs_tokens = []\n",
    "\n",
    "for doc in docs:\n",
    "    tmp_tokens = [token.lemma_ for token in nlp(doc) \n",
    "                  if not token.is_stop\n",
    "                  and not token.is_space\n",
    "                  and not token.is_punct\n",
    "                  and not token.is_oov\n",
    "                  and len(token.lemma_) > 1]\n",
    "    docs_tokens.append(tmp_tokens)\n",
    "    tmp_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --+ let's store the tokenized text\n",
    "df.loc[:, 'tkn_text'] = docs_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get bigrammed-sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some gensim \n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09:37:08: collecting all words and their counts\n",
      "INFO - 09:37:08: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 09:37:09: PROGRESS: at sentence #10000, processed 596681 words and 601 word types\n",
      "INFO - 09:37:09: PROGRESS: at sentence #20000, processed 1194496 words and 619 word types\n",
      "INFO - 09:37:10: PROGRESS: at sentence #30000, processed 1784787 words and 634 word types\n",
      "INFO - 09:37:10: collected 646 word types from a corpus of 2240451 words (unigram + bigrams) and 37688 sentences\n",
      "INFO - 09:37:10: using 646 counts as vocab in Phrases<0 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(docs, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09:40:36: collecting all words and their counts\n",
      "INFO - 09:40:36: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 09:40:36: PROGRESS: at sentence #10000, processed 51031 words and 38711 word types\n",
      "INFO - 09:40:36: PROGRESS: at sentence #20000, processed 102110 words and 66533 word types\n",
      "INFO - 09:40:36: PROGRESS: at sentence #30000, processed 152571 words and 89328 word types\n",
      "INFO - 09:40:36: collected 104537 word types from a corpus of 191315 words (unigram + bigrams) and 37688 sentences\n",
      "INFO - 09:40:36: using 104537 counts as vocab in Phrases<0 vocab, min_count=50, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "# --+ get rid of common terms\n",
    "common_terms = [u'of', u'with', u'without', u'and', u'or', u'the', u'a',\n",
    "                u'not', 'be', u'to', u'this', u'who', u'in']\n",
    "\n",
    "# --+ fing phrases as bigrams\n",
    "bigram = Phrases(docs_tokens,\n",
    "                 min_count=50,\n",
    "                 # max_vocab_size=50000,\n",
    "                 common_terms=common_terms)\n",
    "\n",
    "# --+ manipulate docs\n",
    "docs_phrased = [bigram[line] for line in docs_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the $\\texttt{word2vec}$ with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to speed things up a little bit\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "# load gensim implementation of the word2vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fixing the params requires significant knowledge about the corpus of text at\n",
    "hand and the cultural and societal context for the corpus\n",
    "'''\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09:48:42: collecting all words and their counts\n",
      "INFO - 09:48:42: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 09:48:42: PROGRESS: at sentence #10000, processed 50797 words, keeping 7053 word types\n",
      "INFO - 09:48:42: PROGRESS: at sentence #20000, processed 101661 words, keeping 9684 word types\n",
      "INFO - 09:48:42: PROGRESS: at sentence #30000, processed 151920 words, keeping 11405 word types\n",
      "INFO - 09:48:42: collected 12352 word types from a corpus of 190504 raw words and 37688 sentences\n",
      "INFO - 09:48:42: Loading a fresh vocabulary\n",
      "INFO - 09:48:42: effective_min_count=20 retains 1317 unique words (10% of original 12352, drops 11035)\n",
      "INFO - 09:48:42: effective_min_count=20 leaves 151104 word corpus (79% of original 190504, drops 39400)\n",
      "INFO - 09:48:42: deleting the raw counts dictionary of 12352 items\n",
      "INFO - 09:48:42: sample=6e-05 downsamples 1134 most-common words\n",
      "INFO - 09:48:42: downsampling leaves estimated 45895 word corpus (30.4% of prior 151104)\n",
      "INFO - 09:48:42: estimated required memory for 1317 words and 300 dimensions: 3819300 bytes\n",
      "INFO - 09:48:42: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(docs_phrased, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 09:50:16: Effective 'alpha' higher than previous training cycles\n",
      "INFO - 09:50:16: training model with 7 workers on 1317 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:16: EPOCH - 1 : training on 190504 raw words (45684 effective words) took 0.1s, 413674 effective words/s\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:16: EPOCH - 2 : training on 190504 raw words (46006 effective words) took 0.1s, 406166 effective words/s\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:16: EPOCH - 3 : training on 190504 raw words (45962 effective words) took 0.1s, 399220 effective words/s\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:17: EPOCH - 4 : training on 190504 raw words (45827 effective words) took 0.1s, 389543 effective words/s\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:17: EPOCH - 5 : training on 190504 raw words (45752 effective words) took 0.1s, 405210 effective words/s\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:17: EPOCH - 6 : training on 190504 raw words (45766 effective words) took 0.1s, 382000 effective words/s\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:17: EPOCH - 7 : training on 190504 raw words (45915 effective words) took 0.1s, 404025 effective words/s\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:17: EPOCH - 8 : training on 190504 raw words (45593 effective words) took 0.1s, 370848 effective words/s\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:17: EPOCH - 9 : training on 190504 raw words (45869 effective words) took 0.1s, 354308 effective words/s\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:17: EPOCH - 10 : training on 190504 raw words (45851 effective words) took 0.1s, 337996 effective words/s\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:17: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:17: EPOCH - 11 : training on 190504 raw words (45903 effective words) took 0.1s, 428818 effective words/s\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:18: EPOCH - 12 : training on 190504 raw words (45911 effective words) took 0.1s, 469566 effective words/s\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09:50:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:18: EPOCH - 13 : training on 190504 raw words (45985 effective words) took 0.1s, 381923 effective words/s\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:18: EPOCH - 14 : training on 190504 raw words (46106 effective words) took 0.1s, 414890 effective words/s\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:18: EPOCH - 15 : training on 190504 raw words (46005 effective words) took 0.1s, 416637 effective words/s\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:18: EPOCH - 16 : training on 190504 raw words (45983 effective words) took 0.1s, 389346 effective words/s\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:18: EPOCH - 17 : training on 190504 raw words (45816 effective words) took 0.1s, 384224 effective words/s\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:18: EPOCH - 18 : training on 190504 raw words (45793 effective words) took 0.1s, 442689 effective words/s\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:18: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:18: EPOCH - 19 : training on 190504 raw words (45937 effective words) took 0.1s, 408768 effective words/s\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:19: EPOCH - 20 : training on 190504 raw words (46009 effective words) took 0.1s, 556653 effective words/s\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:19: EPOCH - 21 : training on 190504 raw words (45820 effective words) took 0.1s, 408180 effective words/s\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:19: EPOCH - 22 : training on 190504 raw words (45995 effective words) took 0.1s, 422017 effective words/s\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:19: EPOCH - 23 : training on 190504 raw words (45888 effective words) took 0.1s, 410467 effective words/s\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:19: EPOCH - 24 : training on 190504 raw words (45908 effective words) took 0.1s, 398840 effective words/s\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:19: EPOCH - 25 : training on 190504 raw words (45884 effective words) took 0.1s, 385906 effective words/s\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 5 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09:50:19: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:19: EPOCH - 26 : training on 190504 raw words (46147 effective words) took 0.1s, 406650 effective words/s\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:19: EPOCH - 27 : training on 190504 raw words (45762 effective words) took 0.1s, 404775 effective words/s\n",
      "INFO - 09:50:19: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:20: EPOCH - 28 : training on 190504 raw words (45991 effective words) took 0.1s, 402641 effective words/s\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:20: EPOCH - 29 : training on 190504 raw words (45780 effective words) took 0.1s, 407677 effective words/s\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 09:50:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 09:50:20: EPOCH - 30 : training on 190504 raw words (45847 effective words) took 0.1s, 419700 effective words/s\n",
      "INFO - 09:50:20: training on a 5715120 raw words (1376695 effective words) took 3.7s, 370685 effective words/s\n",
      "INFO - 09:50:20: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.06 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(docs_phrased, total_examples=w2v_model.corpus_count,\n",
    "                epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do vectors make any sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which are the words/meanings the lexical item 'sheldon' trigger in your mind?**\n",
    "\n",
    "<img src=\"images/_14.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        \n",
      "        Lexical item is `sheldon'\n",
      "        ======================================================\n",
      "         term                 similarity\n",
      "        ------------------------------------------------------\n",
      "        \n",
      "\t raj             \t 0.854\n",
      "\t sorry           \t 0.81\n",
      "\t sweetie         \t 0.796\n",
      "\t alright         \t 0.786\n",
      "\t fine            \t 0.782\n",
      "\t oh_god          \t 0.774\n",
      "\t sarcasm         \t 0.773\n",
      "\t okay            \t 0.773\n",
      "\t didn            \t 0.772\n",
      "\t want            \t 0.77\n",
      "\n",
      "        \n",
      "        Lexical item is `sheldon_cooper'\n",
      "        ======================================================\n",
      "         term                 similarity\n",
      "        ------------------------------------------------------\n",
      "        \n",
      "\t dr              \t 0.909\n",
      "\t welcome         \t 0.905\n",
      "\t hofstadter      \t 0.854\n",
      "\t present         \t 0.833\n",
      "\t neighbour       \t 0.796\n",
      "\t flag            \t 0.768\n",
      "\t cooper          \t 0.754\n",
      "\t hello           \t 0.749\n",
      "\t fun             \t 0.744\n",
      "\t honour          \t 0.728\n",
      "\n",
      "        \n",
      "        Lexical item is `dr_sheldon_cooper'\n",
      "        ======================================================\n",
      "        \n",
      "        \n",
      "        !!! ...too bad, item not in dictionary !!!\n",
      "        \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Let's give a try...\n",
    "\n",
    "items = ['sheldon', 'sheldon_cooper', 'dr_sheldon_cooper']\n",
    "\n",
    "for item in items:\n",
    "    try:\n",
    "        positives = w2v_model.wv.most_similar(positive=[item])\n",
    "        print(\"\"\"\n",
    "        \n",
    "        Lexical item is `{}'\n",
    "        ======================================================\n",
    "         term                 similarity\n",
    "        ------------------------------------------------------\n",
    "        \"\"\".format(item))\n",
    "        for term, similarity in positives:\n",
    "            print('\\t', term.ljust(15), '\\t', np.round(similarity, 3), flush=True)\n",
    "    except:\n",
    "        print(\"\"\"\n",
    "        \n",
    "        Lexical item is `{}'\n",
    "        ======================================================\n",
    "        \n",
    "        \n",
    "        !!! ...too bad, item not in dictionary !!!\n",
    "        \n",
    "        \"\"\".format(item))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closer characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = ['sheldon', 'penny', 'leonard', 'howard', 'raj']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**??*\n",
    "\n",
    "<img src=\"images/_15.jpg\" width=\"50%\">\n",
    "\n",
    "*OR*\n",
    "\n",
    "<img src=\"images/_16.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \n",
      "Inter-lexical item similarity\n",
      "======================================================\n",
      "  pair                             similarity\n",
      "------------------------------------------------------\n",
      "\n",
      " sheldon - penny                0.7068729996681213\n",
      " sheldon - leonard              0.7267926335334778\n",
      " sheldon - howard               0.6681523323059082\n",
      " sheldon - raj                  0.8539205193519592\n",
      " penny - sheldon                0.7068729996681213\n",
      " penny - leonard                0.8276159763336182\n",
      " penny - howard                 0.7030239701271057\n",
      " penny - raj                    0.7678197622299194\n",
      " leonard - sheldon              0.7267926335334778\n",
      " leonard - penny                0.8276159763336182\n",
      " leonard - howard               0.7147091031074524\n",
      " leonard - raj                  0.7717246413230896\n",
      " howard - sheldon               0.6681523323059082\n",
      " howard - penny                 0.7030239701271057\n",
      " howard - leonard               0.7147091031074524\n",
      " howard - raj                   0.7719296216964722\n",
      " raj - sheldon                  0.8539205193519592\n",
      " raj - penny                    0.7678197622299194\n",
      " raj - leonard                  0.7717246413230896\n",
      " raj - howard                   0.7719296216964722\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"        \n",
    "Inter-lexical item similarity\n",
    "======================================================\n",
    "  pair                             similarity\n",
    "------------------------------------------------------\n",
    "\"\"\")\n",
    "\n",
    "for c_i in characters:\n",
    "    for c_j in characters:\n",
    "        if c_i != c_j:\n",
    "            sim_cicj = w2v_model.wv.similarity(c_i, c_j)\n",
    "            print(\"\"\"\"\"\",\n",
    "                  '{} - {}'.format(c_i, c_j).ljust(30),\n",
    "                  '{}'.format(sim_cicj))\n",
    "        else:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical post-processing of word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:22:14: saving Word2Vec object under data/big_bang_theory.model, separately None\n",
      "INFO - 11:22:14: not storing attribute vectors_norm\n",
      "INFO - 11:22:14: not storing attribute cum_table\n",
      "INFO - 11:22:15: saved data/big_bang_theory.model\n"
     ]
    }
   ],
   "source": [
    "# save out model\n",
    "out_f = os.path.join('data', 'big_bang_theory.model')\n",
    "w2v_model.save(out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 11:22:57: loading Word2Vec object from data/big_bang_theory.model\n",
      "INFO - 11:22:57: loading wv recursively from data/big_bang_theory.model.wv.* with mmap=None\n",
      "INFO - 11:22:57: setting ignored attribute vectors_norm to None\n",
      "INFO - 11:22:57: loading vocabulary recursively from data/big_bang_theory.model.vocabulary.* with mmap=None\n",
      "INFO - 11:22:57: loading trainables recursively from data/big_bang_theory.model.trainables.* with mmap=None\n",
      "INFO - 11:22:57: setting ignored attribute cum_table to None\n",
      "INFO - 11:22:57: loaded data/big_bang_theory.model\n"
     ]
    }
   ],
   "source": [
    "# load the data back\n",
    "in_f = out_f\n",
    "model = Word2Vec.load(in_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.56122034e-02,  5.70277534e-02,  4.97568101e-02,  8.43853131e-02,\n",
       "       -1.51177630e-01,  9.56035405e-03,  2.75666080e-02,  2.51432359e-02,\n",
       "       -2.45556533e-02, -1.44383898e-02, -7.56913200e-02, -4.07761298e-02,\n",
       "        5.18479533e-02, -2.74627507e-02, -1.07280761e-01, -9.56522375e-02,\n",
       "       -1.11931354e-01,  8.44372902e-03, -5.73443733e-02, -3.99773158e-02,\n",
       "        9.35376659e-02, -1.08790612e-02,  6.88532069e-02, -5.39100245e-02,\n",
       "        8.92639756e-02, -4.24373187e-02, -5.31451702e-02,  3.04649607e-03,\n",
       "       -1.79845113e-02, -1.28390081e-02,  1.00214235e-01, -1.17637916e-03,\n",
       "       -1.27293199e-01, -2.69187130e-02,  4.98978756e-02,  1.22400016e-01,\n",
       "        2.84108194e-03, -1.30958766e-01, -6.02252483e-02, -2.43704882e-03,\n",
       "       -6.80608675e-02, -1.39539884e-02,  8.00053179e-02,  1.03007294e-01,\n",
       "       -2.79188193e-02, -7.66695067e-02,  3.27624157e-02,  6.69604540e-02,\n",
       "        4.46786545e-03, -7.33269705e-03, -5.39108254e-02, -6.13568397e-03,\n",
       "        8.85235816e-02, -3.42095494e-02,  7.70579875e-02, -6.09187782e-02,\n",
       "        5.39092496e-02, -7.93745071e-02,  2.84263492e-02,  7.00196177e-02,\n",
       "        7.04451054e-02,  1.09400325e-01,  1.16033427e-01,  4.71552908e-02,\n",
       "        6.88151792e-02, -1.02343872e-01,  4.01311880e-03,  2.47315504e-02,\n",
       "       -2.64792377e-03, -1.74808856e-02,  1.02108531e-01,  2.53580362e-02,\n",
       "       -5.34626469e-02, -4.46610563e-02, -2.15354878e-02,  7.84460381e-02,\n",
       "        8.59723836e-02, -5.60754735e-04,  6.87880591e-02, -2.56551355e-02,\n",
       "       -5.77072911e-02, -4.56718076e-03, -1.97295635e-03, -6.99402392e-02,\n",
       "        1.80206914e-03, -9.48827155e-03, -9.64683108e-03,  3.76579985e-02,\n",
       "        1.52916154e-02,  1.30586959e-02,  4.84303497e-02,  7.81794265e-02,\n",
       "       -2.35807654e-02,  5.89217134e-02, -3.70460488e-02,  2.61928700e-02,\n",
       "       -2.45448556e-02, -9.32867825e-03,  1.54240012e-01,  3.21105532e-02,\n",
       "       -2.09608469e-02,  2.00553844e-03, -2.87975501e-02,  1.62478425e-02,\n",
       "       -2.70628836e-03, -3.67207713e-02,  8.41717795e-02,  1.06928498e-01,\n",
       "       -5.53291962e-02, -5.55314459e-02,  7.35306814e-02, -5.47670871e-02,\n",
       "       -2.06376147e-02,  4.29493077e-02,  1.59198791e-02,  2.39534266e-02,\n",
       "        7.01229572e-02,  1.11386767e-02, -3.10894754e-02,  1.61892585e-02,\n",
       "       -5.78175858e-02,  4.33493638e-03, -1.62614253e-03, -1.66119896e-02,\n",
       "        1.11753109e-03, -6.48852363e-02,  1.36722252e-02,  6.26662150e-02,\n",
       "       -2.67330483e-02,  1.05292462e-01, -6.56020716e-02,  4.48784232e-02,\n",
       "        4.10432629e-02, -1.19210459e-01, -3.97469942e-03,  7.50691816e-02,\n",
       "        1.05548881e-01, -2.39529163e-02, -8.34542289e-02, -6.06868532e-04,\n",
       "        1.05644567e-02, -5.08974679e-02, -5.58628254e-02, -1.67714451e-02,\n",
       "       -3.13621722e-02, -5.04188351e-02, -4.60510217e-02, -5.84154343e-03,\n",
       "       -1.28573507e-01, -1.95142590e-02, -3.38395573e-02,  2.15557069e-02,\n",
       "        5.08438647e-02,  5.24515547e-02,  6.15956485e-02, -4.50405553e-02,\n",
       "        1.00228861e-01,  4.73844819e-02, -3.92878763e-02,  3.80829088e-02,\n",
       "        2.21317541e-02,  5.05628996e-02,  1.68786421e-02, -6.04358502e-02,\n",
       "       -5.37556075e-02,  6.13892600e-02,  9.57509968e-03, -3.82545963e-02,\n",
       "       -9.37476158e-02,  9.23531950e-02,  5.48214093e-02,  4.38646004e-02,\n",
       "       -1.18850488e-02,  1.69667564e-02,  9.25200582e-02, -4.72001405e-03,\n",
       "        1.43115688e-02, -1.34858757e-03, -7.13620931e-02, -6.18368350e-02,\n",
       "        6.30284026e-02, -5.26800081e-02, -3.92570905e-02,  6.45148242e-03,\n",
       "       -1.65884703e-01,  2.57660355e-02, -9.07298028e-02, -3.53479609e-02,\n",
       "        3.47962807e-04, -1.27917789e-02,  1.79968346e-02,  5.89123070e-02,\n",
       "        5.68117313e-02, -7.10743219e-02,  4.82624620e-02,  3.81962247e-02,\n",
       "       -1.09757565e-01, -6.02091895e-03, -6.49702270e-05,  6.31990507e-02,\n",
       "       -1.02220559e-02, -2.03447342e-02, -4.45730723e-02,  7.97062144e-02,\n",
       "       -7.39667937e-02,  2.93181557e-02,  1.00097969e-01, -3.47554386e-02,\n",
       "       -1.13666710e-02,  1.89536251e-02, -2.53700055e-02, -4.00182828e-02,\n",
       "        1.82228163e-02, -1.02305889e-01,  4.67316769e-02, -7.28638982e-03,\n",
       "        1.36540998e-02,  5.09444019e-03, -1.12165161e-03,  6.88755363e-02,\n",
       "       -3.90797742e-02,  1.03642136e-01,  1.39442772e-01,  3.62804830e-02,\n",
       "       -4.70021507e-03, -2.48301704e-03,  3.19713587e-03,  7.81601816e-02,\n",
       "        7.92499334e-02,  5.46554551e-02,  1.29804872e-02, -2.69116964e-02,\n",
       "       -6.65191710e-02, -1.22475378e-01,  6.53205812e-02,  1.88403800e-02,\n",
       "       -6.33191969e-03,  6.00183010e-02, -4.97677512e-02, -1.45443650e-02,\n",
       "        6.79761693e-02,  6.76160008e-02,  1.05601326e-01,  5.12647163e-03,\n",
       "       -1.79573447e-02, -1.06935538e-01, -3.49937081e-02, -1.11582756e-01,\n",
       "        6.40873909e-02,  4.00272273e-02,  8.59023631e-02,  5.61649119e-03,\n",
       "       -3.61451544e-02,  8.90196115e-03,  1.67675633e-02, -2.35410575e-02,\n",
       "       -4.74855043e-02, -2.43269447e-02, -2.74483766e-02, -7.74543285e-02,\n",
       "       -1.37085885e-01, -1.78501941e-03, -6.69184178e-02, -1.89223979e-02,\n",
       "        3.68571049e-03,  1.30984215e-02, -7.70089850e-02,  1.99815035e-02,\n",
       "       -4.83751670e-02,  4.29264903e-02, -6.35689646e-02,  2.78502963e-02,\n",
       "       -6.12210250e-03, -2.17414219e-02,  6.00006171e-02, -1.27042666e-01,\n",
       "        1.87940374e-02,  6.54756799e-02,  4.84238714e-02,  8.85397196e-02,\n",
       "        5.03939465e-02,  6.78592101e-02,  1.01647880e-02,  4.12208252e-02,\n",
       "        3.35146785e-02, -4.70304154e-02, -1.05164398e-03, -5.98725528e-02,\n",
       "       -3.43533233e-03,  1.54618509e-02, -5.98790720e-02,  3.27820033e-02,\n",
       "       -4.05960940e-02, -3.09860874e-02, -2.08424795e-02, -9.22776759e-02,\n",
       "        1.28294658e-02, -4.31200005e-02,  3.09795112e-04, -1.52754188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['sheldon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's store the word vectors associated with target items\n",
    "character_vectors = []\n",
    "\n",
    "for c in characters:\n",
    "    to_append = model.wv[c]\n",
    "    character_vectors.append(to_append)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"mark\">Students have time to run dimensionality reduction analyses</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "410px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
