{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 3, part II\n",
    "\n",
    "## Vector semantics and embeddings\n",
    "\n",
    "<img src=\"images/_0.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Learning goals\n",
    "============\n",
    "\n",
    "\n",
    "- Appreciating the anatomy of $\\texttt{word2vec}$ ⬅️\n",
    "- Understanding the various steps to train ad hoc embeddings  ⬅️\n",
    "- Assigning a role to new forms of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\texttt{word2vec}$: the basic\n",
    "===================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Key features**\n",
    "\n",
    "+ in 2013, it drew a line between old-school and modern NLP\n",
    "+ it doesn't require hand-labeled supervision\n",
    "+ easy and quite fast to train (you can do that with Gensim)\n",
    "+ it's OSS  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Philosophy**\n",
    "\n",
    "+ it trains a classifier on a binary prediction task:\n",
    "  - \"is word $\\omega$ likely to show up near word $\\eta$\"?\n",
    "+ the classification task is 'instrumental' in nature:\n",
    "  - the point is not predicting the 'next' word\n",
    "  - the goal is to adjust word vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's focus on $\\texttt{word2vec}$: Background\n",
    "==================================\n",
    "\n",
    "<img src=\"images/_4.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\texttt{word2vec}$ algorithm: Skip-Gram flavor\n",
    "==================================\n",
    "\n",
    "**!!!Boundary condition¡¡¡**\n",
    "\n",
    "+ there are various flavors of the $\\texttt{word2vec}$\n",
    "+ here, we focus on the Skip-Gram (SG) flavor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Skip-Gram algorithm\n",
    "=================\n",
    "\n",
    "1. treat the target word and a neighboring context word as positive examples\n",
    "2. randomly sample other words in the lexicon to get negative examples\n",
    "3. use logistic regression to train a classifier to distinguish those two cases\n",
    "4. use the weights as the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "SG training data\n",
    "==============\n",
    "\n",
    "Given the sentence:\n",
    "\n",
    "| not in context | c1         | c2 | t       | c3  | c4 | not in context | \n",
    "| -------------- |------------|----|---------|-----|----|----------------|\n",
    "| ... lemon, a   | tablespoon | of | apricot | jam | a  | pinch of ...   |\n",
    "\n",
    "let's assume context words are those in +/- 2 word window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "SG goal\n",
    "=======\n",
    " \n",
    "Given a tuple $(t,c)  = target, context$\n",
    "\n",
    "+ $\\texttt{(apricot, jam)}$\n",
    "+ $\\texttt{(apricot, aardvark)}$\n",
    "\n",
    "Return probability that $c$ is a real context word:\n",
    "\n",
    "$P(+|t,c)$\n",
    "\n",
    "$P(-|t,c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How to compute p(+|t,c)?\n",
    "=====================\n",
    "\n",
    "Intuition:\n",
    "\n",
    "+ words are likely to appear near similar words\n",
    "+ model similarity with dot-product!\n",
    "+ similarity(t,c)  $∝ t ∙ c$\n",
    "\n",
    "Problem:\n",
    "+ dot product is not a probability!\n",
    "+ (neither is cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Turning dot product into a probability\n",
    "===============================\n",
    "\n",
    "\\begin{equation}\n",
    "P(+|t,c) = \\frac{1}{1+e^{-t ∙ c}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(-|t,c) = 1 - P(+|t,c)\n",
    "= \\frac{e^{-t ∙ c}}{1+e^{-t ∙ c}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For all context words\n",
    "=================\n",
    "\n",
    "\\begin{equation}\n",
    "P(+|t,c_{1:k}) = \\prod_{i=1}^{k} \\frac{1}{1+e^{-t ∙ c_{i}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "SG training data\n",
    "=============\n",
    "Given the sentence:\n",
    "\n",
    "| not in context | c1         | c2 | t       | c3  | c4 | not in context | \n",
    "| -------------- |------------|----|---------|-----|----|----------------|\n",
    "| ... lemon, a   | tablespoon | of | apricot | jam | a  | pinch of ...   |\n",
    "\n",
    "let's assume context words are those in +/- 2 word window.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "SG training (1/2)\n",
    "============== \n",
    "\n",
    "Given the sentence:\n",
    "\n",
    "| not in context | c1         | c2 | t       | c3  | c4 | not in context | \n",
    "| -------------- |------------|----|---------|-----|----|----------------|\n",
    "| ... lemon, a   | tablespoon | of | apricot | jam | a  | pinch of ...   |\n",
    "\n",
    "we look for positive examples $+$:\n",
    "\n",
    "| t       | c          |\n",
    "|---------| -----------|\n",
    "| apricot | tablespoon |\n",
    "| apricot | of         |\n",
    "| apricot | jam        |\n",
    "| apricot | preserve   |\n",
    "| apricot | ...        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "SG training (2/2)\n",
    "============== \n",
    "\n",
    "Given the list of + $c$:\n",
    "\n",
    "+ each positive $c$ is matched with a negative $c$\n",
    "+ negatives are 'noise words' that do not belong to any linguistic contexts of $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Setup\n",
    "=====\n",
    "\n",
    "Let's represent words as vectors of some length (say 300), randomly initialized. \n",
    "\n",
    "+ we start with 300 * V random parameters\n",
    "+ over the entire training set, we’d like to adjust those word vectors such that we:\n",
    "  + maximize the similarity of the target word, context word pairs (t,c) drawn from the positive data\n",
    "  + minimize the similarity of the (t,c) pairs drawn from the negative data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Learning $\\texttt{word2vec}$ embeddings\n",
    "=============================\n",
    "\n",
    "<img src=\"images/_6.png\" width=\"80%\">\n",
    "\n",
    "Source is Jurafsky and Martin (2019)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
